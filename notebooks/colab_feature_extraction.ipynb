{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fa3674dae43f6eb",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc1694c785b29de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import queue\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from google.cloud import storage\n",
    "from google.colab import auth \n",
    "import h5py \n",
    "import numpy as np \n",
    "import time #\n",
    "\n",
    "from extract_vision_features import process_single_wsi, load_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571347d9606e9d3",
   "metadata": {},
   "source": [
    "# Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f96ef24c46e4494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration (UPDATE THESE VALUES) ---\n",
    "GCP_PROJECT_ID = \"your-gcp-project-id\" # <--- IMPORTANT: REPLACE WITH YOUR GCP PROJECT ID\n",
    "GCS_BUCKET_NAME = \"histo-bench\"\n",
    "GCS_WSI_FOLDER = \"TCGA-LGG/wsi/\"\n",
    "GCS_COORDINATES_FOLDER = \"TCGA-LGG/coordinates/\" \n",
    "\n",
    "LOCAL_WSI_DOWNLOAD_DIR = \"/content/wsi_temp_downloads/\"\n",
    "LOCAL_COORDINATES_DIR = \"/content/coordinates/\"\n",
    "\n",
    "CONFIG_PATH = \"\"\n",
    "\n",
    "BATCH_SIZE_WSI_LISTING = 5 \n",
    "MAX_DOWNLOAD_WORKERS = 3 # Number of concurrent downloads (adjust based on network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672f18c5e991dc6e",
   "metadata": {},
   "source": [
    "# Authentication "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4a77cf4ad9a7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 0. Colab Authentication and Project Setup ---\n",
    "print(\"--- Authenticating Google Colab and Setting GCP Project ---\")\n",
    "try:\n",
    "    auth.authenticate_user()\n",
    "    print(\"Colab authenticated.\")\n",
    "    os.environ['GCLOUD_PROJECT'] = GCP_PROJECT_ID\n",
    "    !gcloud config set project {GCP_PROJECT_ID}\n",
    "except Exception as e:\n",
    "    print(f\"Authentication or project setup failed: {e}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# --- 1. Download Patch Coordinates ---\n",
    "print(f\"\\n--- Downloading Patch Coordinate Files from {GCS_BUCKET_NAME}/{GCS_COORDINATES_FOLDER} ---\")\n",
    "os.makedirs(LOCAL_COORDINATES_DIR, exist_ok=True)\n",
    "!gcloud storage cp gs://{GCS_BUCKET_NAME}/{GCS_COORDINATES_FOLDER}*.h5 {LOCAL_COORDINATES_DIR}\n",
    "print(f\"Coordinate files downloaded to: {LOCAL_COORDINATES_DIR}\")\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def get_wsi_batches_from_gcs(bucket_name: str, gcs_folder_path: str, batch_size: int = 5):\n",
    "    \"\"\"\n",
    "    Lists .svs files from a specified GCS folder path and yields them in batches.\n",
    "    \"\"\"\n",
    "    # Ensure gcs_folder_path ends with a slash if it's not empty, for proper prefix matching\n",
    "    if gcs_folder_path and not gcs_folder_path.endswith('/'):\n",
    "        gcs_folder_path += '/'\n",
    "\n",
    "    print(f\"Listing files in bucket: {bucket_name}, folder: {gcs_folder_path}\")\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "    all_wsi_blobs = [\n",
    "        blob for blob in bucket.list_blobs(prefix=gcs_folder_path)\n",
    "        if blob.name.endswith('.svs')\n",
    "    ]\n",
    "\n",
    "    if not all_wsi_blobs:\n",
    "        print(f\"No .svs files found in gs://{bucket_name}/{gcs_folder_path}. Exiting.\")\n",
    "        return # Exit if no files are found\n",
    "\n",
    "    print(f\"Found {len(all_wsi_blobs)} WSI files.\")\n",
    "    # Sort blobs by name for consistent batching across runs\n",
    "    all_wsi_blobs.sort(key=lambda blob: blob.name)\n",
    "\n",
    "    current_batch = []\n",
    "    for blob in all_wsi_blobs:\n",
    "        full_gcs_path = f\"gs://{bucket_name}/{blob.name}\"\n",
    "        current_batch.append(full_gcs_path)\n",
    "        if len(current_batch) == batch_size:\n",
    "            yield current_batch\n",
    "            current_batch = []\n",
    "    # Yield any remaining files in the last batch\n",
    "    if current_batch:\n",
    "        yield current_batch\n",
    "\n",
    "def download_single_svs_file(bucket_name: str, gcs_file_path: str, local_download_dir: str, download_queue: queue.Queue):\n",
    "    \"\"\"\n",
    "    Downloads a single .svs file from GCS and puts its local path into a queue.\n",
    "    This function is designed to be run in a separate thread.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        os.makedirs(local_download_dir, exist_ok=True)\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "        blob_name = gcs_file_path.replace(f\"gs://{bucket_name}/\", \"\")\n",
    "        blob = bucket.blob(blob_name)\n",
    "        destination_file_name = os.path.join(local_download_dir, os.path.basename(blob.name))\n",
    "\n",
    "        print(f\"[DOWNLOAD] Starting download of {blob.name} to {destination_file_name}\")\n",
    "        blob.download_to_filename(destination_file_name)\n",
    "        print(f\"[DOWNLOAD] Finished download of {blob.name}\")\n",
    "\n",
    "        download_queue.put(destination_file_name)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Error downloading {gcs_file_path}: {e}\")\n",
    "        download_queue.put(None) # Signal failure for this specific download\n",
    "\n",
    "def get_coordinate_file_path(wsi_filename: str, local_coordinates_dir: str):\n",
    "    \"\"\"\n",
    "    Determines the expected local path of the .h5 coordinate file for a given WSI filename.\n",
    "    Assumes coordinate files have the same base name as WSI but with .h5 extension.\n",
    "    E.g., TCGA-XX-YYYY.svs -> TCGA-XX-YYYY.h5\n",
    "    \"\"\"\n",
    "    base_name = os.path.splitext(os.path.basename(wsi_filename))[0] # Get filename without .svs from full path\n",
    "    coord_filename = f\"{base_name}.h5\"\n",
    "    full_coord_path = os.path.join(local_coordinates_dir, coord_filename)\n",
    "    return full_coord_path\n",
    "\n",
    "# --- Main processing loop structure ---\n",
    "def main_batch_processor(\n",
    "    bucket_name: str,\n",
    "    gcs_wsi_folder: str,\n",
    "    local_wsi_download_dir: str,\n",
    "    batch_size_wsi_listing: int = 5,\n",
    "    max_download_workers: int = 3\n",
    "):\n",
    "    \"\"\"\n",
    "    Main function to orchestrate concurrent downloading and processing of WSI files.\n",
    "    \"\"\"\n",
    "    os.makedirs(local_wsi_download_dir, exist_ok=True)\n",
    "    download_queue = queue.Queue() # Queue to hold paths of downloaded files\n",
    "\n",
    "    print(\"\\n--- Starting Concurrent WSI Processing ---\")\n",
    "\n",
    "    # Get total number of files to process for completion check\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    total_files_to_process = len([\n",
    "        blob for blob in bucket.list_blobs(prefix=gcs_wsi_folder)\n",
    "        if blob.name.endswith('.svs')\n",
    "    ])\n",
    "    print(f\"Total WSI files identified for processing: {total_files_to_process}\")\n",
    "\n",
    "    # Use a ThreadPoolExecutor for parallel downloads\n",
    "    with ThreadPoolExecutor(max_workers=max_download_workers) as download_executor:\n",
    "        download_futures = [] # To keep track of submitted download tasks\n",
    "\n",
    "        # Submit all download tasks concurrently in the background\n",
    "        for batch_gcs_paths in get_wsi_batches_from_gcs(bucket_name, gcs_wsi_folder, batch_size_wsi_listing):\n",
    "            for gcs_path in batch_gcs_paths:\n",
    "                future = download_executor.submit(\n",
    "                    download_single_svs_file,\n",
    "                    bucket_name,\n",
    "                    gcs_path,\n",
    "                    local_wsi_download_dir,\n",
    "                    download_queue\n",
    "                )\n",
    "                download_futures.append(future)\n",
    "\n",
    "        processed_count = 0\n",
    "        # Main thread loop: continuously try to get files from the queue and process them\n",
    "        while processed_count < total_files_to_process:\n",
    "            try:\n",
    "                # Get a downloaded file path from the queue (with a timeout to prevent infinite blocking)\n",
    "                # If queue is empty and downloads are still running, it will wait\n",
    "                local_wsi_file_path = download_queue.get(timeout=100) # Increased timeout\n",
    "                if local_wsi_file_path is None: # Handle potential download errors\n",
    "                    print(\"[MAIN_LOOP] Skipping processing due to a previous download error.\")\n",
    "                    processed_count += 1\n",
    "                    continue\n",
    "\n",
    "                print(f\"\\n[MAIN_LOOP] Processing local file: {local_wsi_file_path}\")\n",
    "\n",
    "                wsi_filename = os.path.basename(local_wsi_file_path)                \n",
    "                config = load_config(CONFIG_PATH)\n",
    "                process_single_wsi(wsi_filename, config)\n",
    "                \n",
    "                # Remove the local WSI data after processing\n",
    "                if os.path.exists(local_wsi_file_path):\n",
    "                    os.remove(local_wsi_file_path)\n",
    "                    print(f\"[CLEANUP] Removed local WSI file: {local_wsi_file_path}\")\n",
    "                else:\n",
    "                    print(f\"[CLEANUP] WSI file not found for cleanup: {local_wsi_file_path}\")\n",
    "\n",
    "                processed_count += 1\n",
    "            except queue.Empty:\n",
    "                # Check if all downloads are truly finished and queue is empty, then break\n",
    "                if all(f.done() for f in download_futures) and download_queue.empty():\n",
    "                    print(\"[MAIN_LOOP] All downloads complete and queue is empty. Exiting processing loop.\")\n",
    "                    break\n",
    "                print(\"[MAIN_LOOP] Queue is temporarily empty, waiting for more downloads or completion...\")\n",
    "                time.sleep(1) # Wait a bit before retrying to avoid busy-waiting\n",
    "            except Exception as e:\n",
    "                print(f\"[MAIN_LOOP] An unexpected error occurred during processing: {e}\")\n",
    "                processed_count += 1 # Ensure loop progresses even on errors\n",
    "\n",
    "    print(\"\\n--- All WSI files processed (or attempted). ---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f954a5348f2bb6",
   "metadata": {},
   "source": [
    "# EXECUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53336000d8b53790",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    os.makedirs(LOCAL_WSI_DOWNLOAD_DIR, exist_ok=True)\n",
    "    os.makedirs(LOCAL_COORDINATES_DIR, exist_ok=True)\n",
    "\n",
    "    main_batch_processor(\n",
    "        bucket_name=GCS_BUCKET_NAME,\n",
    "        gcs_wsi_folder=GCS_WSI_FOLDER,\n",
    "        local_wsi_download_dir=LOCAL_WSI_DOWNLOAD_DIR,\n",
    "        batch_size_wsi_listing=BATCH_SIZE_WSI_LISTING,\n",
    "        max_download_workers=MAX_DOWNLOAD_WORKERS\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea00dabe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5e9190c",
   "metadata": {},
   "source": [
    "# Preprocess Manifest and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7c08c900",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "path =\"/Users/bakhtierzhon.pashshoev/Downloads/gdc_download_20250711_164811.924176/f3a1bc62-9552-4553-b318-7d9c21d21ce7/nationwidechildrens.org_clinical_patient_lgg.txt\"\n",
    "patient_data = pd.read_csv(path, sep=\"\\t\")\n",
    "\n",
    "# Read and select the necessary columns\n",
    "patient_data = patient_data.iloc[2:]\n",
    "selected_columns = [\"bcr_patient_barcode\", \"histologic_diagnosis\", \"tumor_grade\"]\n",
    "patient_data = patient_data[selected_columns].dropna()\n",
    "patient_data.rename(columns={\"bcr_patient_barcode\": \"barcode\"}, inplace=True)\n",
    "\n",
    "# Read the manifest file\n",
    "manifest = pd.read_csv(\"/Users/bakhtierzhon.pashshoev/Desktop/Thesis/TCGA-LGG/gdc_manifest.txt\", sep=\"\\t\")\n",
    "manifest[\"barcode\"] = manifest[\"filename\"].apply(lambda x: \"-\".join(x.split(\"-\")[:3]))\n",
    "\n",
    "# Combine the manifest and patient data\n",
    "combined = manifest.merge(patient_data, on=\"barcode\", how=\"left\")\n",
    "combined = combined[[\"barcode\", \"filename\", \"histologic_diagnosis\", \"tumor_grade\", \"size\"]]\n",
    "combined = combined.dropna()\n",
    "\n",
    "# Keep the largest file for each barcode\n",
    "combined = combined.sort_values(by=[\"barcode\", \"size\"], ascending=[True, False])\n",
    "combined = combined.drop_duplicates(subset=\"barcode\", keep=\"first\")\n",
    "\n",
    "# Add slide_id column\n",
    "combined = combined.assign(slide_id=combined[\"filename\"].apply(lambda x: \".\".join(x.split(\".\")[:-1])))\n",
    "\n",
    "# Shuffle the dataframe\n",
    "combined = combined.sample(frac=1)\n",
    "\n",
    "# Filter manifest to only include slides from combined dataframe\n",
    "filtered_manifest = manifest[manifest[\"filename\"].isin(combined[\"filename\"])]\n",
    "filtered_manifest = filtered_manifest.drop(columns=[\"barcode\"])\n",
    "\n",
    "# Encode the labels for training metadata\n",
    "training_metadata = combined[[\"slide_id\", \"histologic_diagnosis\", \"tumor_grade\", \"barcode\"]]\n",
    "unique_labels = training_metadata['histologic_diagnosis'].unique()\n",
    "label_to_index = {label: i for i, label in enumerate(unique_labels)}\n",
    "training_metadata = training_metadata.assign(label=training_metadata['histologic_diagnosis'].map(label_to_index))\n",
    "\n",
    "# Save the training metadata\n",
    "training_metadata.to_csv(\"training_metadata.csv\", index=False)\n",
    "\n",
    "# Save the filtered manifest for downstream tasks\n",
    "filtered_manifest.to_csv(\"filtered_manifest.txt\", index=False, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa1adac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd118a02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6abdad2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Users/bakhtierzhon.pashshoev/Cursor/histo-bench\")\n",
    "from scripts.models.vision.resnet import ResNetEncoder\n",
    "\n",
    "encoder = ResNetEncoder(device=\"mps\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3295981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_shape': (1, 3, 224, 224),\n",
       " 'output_shape': (1, 2048, 1, 1),\n",
       " 'total_parameters': 23508032}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.get_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbe727b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "histo-bench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
