# Available Models

This document provides an overview of the vision backbones and MIL models available in this project.

## Vision Foundation Models

- **ResNet50** – A standard CNN backbone commonly used as a baseline
- **PLIP** – A lightweight CLIP model
- **UNI** – A transformer model optimized
- **CONCH** – A contrastive learning-based model

## MIL Models

- **MeanPooling** – Simple MIL approach that aggregates instance features via global average pooling.
- **ABMIL** – Attention-based MIL that learns to focus on the most informative patches.
- **CLAM** – A refinement of ABMIL with instance-level supervision; well-suited for weakly labeled data.
- **WiKG** – Graph-based MIL model that captures spatial relationships between patches.
- **TransMIL** – Transformer-based MIL that models long-range dependencies between patches.

## Notes

TBD

## References
TBD

